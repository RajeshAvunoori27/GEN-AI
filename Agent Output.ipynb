{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, getpass\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    # Check if the environment variable is already set\n",
    "    if not os.environ.get(var):\n",
    "        # Prompt the user to enter the value for the environment variable\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# Set the environment variable \"GROQ_API_KEY\"\n",
    "_set_env(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import MessagesState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherResponse(BaseModel):\n",
    "    \"\"\" Respond to the user with this\"\"\"\n",
    "\n",
    "    temperature: float = Field(description='The temperature in fahrenheit')\n",
    "    wind_direction: str = Field(description='The direction of the wind in abbreviated form')\n",
    "    wind_speed: float = Field(description='The Speed of the wind in km/h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    A model representing the weather response.\n",
    "\n",
    "    Attributes:\n",
    "        temperature (float): The temperature in Fahrenheit.\n",
    "        wind_direction (str): The direction of the wind in abbreviated form.\n",
    "        wind_speed (float): The speed of the wind in km/h.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(MessagesState):\n",
    "    # Final structured response from the agent\n",
    "    final_response: WeatherResponse\n",
    "\n",
    "@tool\n",
    "def get_weather(city: Literal['nyc', 'sf']):\n",
    "    \"\"\" Use this to get weather information\"\"\"\n",
    "\n",
    "    # Return weather information based on the city\n",
    "    if city == 'nyc':\n",
    "        return 'It is cloudy in NYC, with 5 mph winds in the North-East direction and a temperature of 70 degrees'\n",
    "    elif city == 'sf':\n",
    "        return \"It is 75 degrees and sunny in SF, with 3 mph winds in the South-East direction\"\n",
    "    else:\n",
    "        raise AssertionError(\"Unknown city\")\n",
    "\n",
    "# List of tools to be used by the model\n",
    "tools = [get_weather]\n",
    "\n",
    "# Initialize the ChatGroq model with a specific configuration\n",
    "model = ChatGroq(model='llama3-8b-8192')\n",
    "\n",
    "# Bind the tools to the model\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "# Configure the model to return structured output\n",
    "model_with_structured_output = model.with_structured_output(WeatherResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single LLM option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# List of tools to be used by the model\n",
    "tools = [get_weather, WeatherResponse]\n",
    "\n",
    "# Force the model to use tools by passing tool_choice = 'any'\n",
    "model_with_response_tool = model.bind_tools(tools, tool_choice='any')\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: AgentState):\n",
    "    # Invoke the model with the current state messages\n",
    "    response = model_with_response_tool.invoke(state['messages'])\n",
    "    # Return the response as a list to be added to the existing messages\n",
    "    return {'messages': [response]}\n",
    "\n",
    "# Define the function that responds to the user\n",
    "def respond(state: AgentState):\n",
    "    # Construct the final answer from the arguments of the last tool call\n",
    "    weather_tool_call = state['messages'][-1].tool_calls[0]\n",
    "    response = WeatherResponse(**weather_tool_call['args'])\n",
    "  \n",
    "    tool_message = {\n",
    "        'type': 'tool',\n",
    "        'content': 'Here is your structured response',\n",
    "        'tool_call_id': weather_tool_call['id'],\n",
    "    }\n",
    "\n",
    "    # Return the final answer and the tool message\n",
    "    return {'final_response': response, 'messages': [tool_message]}\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: AgentState):\n",
    "    message = state['messages']\n",
    "    last_message = message[-1]\n",
    "    # If there is only one tool call and it is the response tool call, respond to the user\n",
    "    if (\n",
    "        len(last_message.tool_calls) == 1\n",
    "        and \n",
    "        last_message.tool_calls[0]['name'] == 'WeatherResponse'\n",
    "    ):\n",
    "        return 'respond'\n",
    "    # Otherwise, continue using the tool node\n",
    "    else:\n",
    "        return 'continue'\n",
    "    \n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model) \n",
    "workflow.add_node(\"respond\", respond)\n",
    "workflow.add_node('tools', ToolNode(tools))\n",
    "\n",
    "# Set the entry point as 'agent'\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point('agent')\n",
    "\n",
    "# Add conditional edges to determine the flow of the graph\n",
    "workflow.add_conditional_edges(\n",
    "    'agent',\n",
    "    should_continue,\n",
    "    {\n",
    "        'continue': 'tools',\n",
    "        'respond': 'respond'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add edges to define the transitions between nodes\n",
    "workflow.add_edge('tools', 'agent')\n",
    "workflow.add_edge('respond', END)\n",
    "\n",
    "# Compile the workflow into a graph\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = graph.invoke(input={\n",
    "                            'messages': [('human',\n",
    "                                        \"what's the weather in SF\")]\n",
    "                            })['final_response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeatherResponse(temperature=75.0, wind_direction='S-E', wind_speed=3.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: AgentState):\n",
    "    # Invoke the model with the current state messages\n",
    "    response = model_with_tools.invoke(state[\"messages\"])\n",
    "    # Return the response as a list to be added to the existing messages\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define the function that responds to the user\n",
    "def respond(state: AgentState):\n",
    "    # Call the model with structured output to return a consistent format to the user\n",
    "    # Convert the last ToolMessage in the conversation to a HumanMessage for the model to use\n",
    "    response = model_with_structured_output.invoke(\n",
    "        [HumanMessage(content=state[\"messages\"][-2].content)]\n",
    "    )\n",
    "    # Return the final answer\n",
    "    return {\"final_response\": response}\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, respond to the user\n",
    "    if not last_message.tool_calls:\n",
    "        return \"respond\"\n",
    "    # Otherwise, continue using the tool node\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"respond\", respond)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Set the entry point as 'agent'\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Add conditional edges to determine the flow of the graph\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"respond\": \"respond\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add edges to define the transitions between nodes\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "workflow.add_edge(\"respond\", END)\n",
    "\n",
    "# Compile the workflow into a graph\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = graph.invoke(input={\"messages\": [(\"human\", \"what's the weather in SF?\")]})[\n",
    "    \"final_response\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeatherResponse(temperature=75.0, wind_direction='S-E', wind_speed=3.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
